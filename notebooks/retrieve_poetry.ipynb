{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ce5fb2",
   "metadata": {},
   "source": [
    "# Retrieve Poetry\n",
    "## Poetry Retriever using the Poly-encoder Transformer architecture (Humeau et al., 2019) for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed6b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is based on :\n",
    "# https://aritter.github.io/CS-7650/\n",
    "# This Project was developed at the Georgia Institute of Technology by Ashutosh Baheti (ashutosh.baheti@cc.gatech.edu), \n",
    "# borrowing  from the Neural Machine Translation Project (Project 2) \n",
    "# of the UC Berkeley NLP course https://cal-cs288.github.io/sp20/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6490ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "import statistics\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import tqdm\n",
    "import nltk\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe91f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General util functions\n",
    "def make_dir_if_not_exists(directory):\n",
    "\tif not os.path.exists(directory):\n",
    "\t\tlogging.info(\"Creating new directory: {}\".format(directory))\n",
    "\t\tos.makedirs(directory)\n",
    "\n",
    "def print_list(l, K=None):\n",
    "\t# If K is given then only print first K\n",
    "\tfor i, e in enumerate(l):\n",
    "\t\tif i == K:\n",
    "\t\t\tbreak\n",
    "\t\tprint(e)\n",
    "\tprint()\n",
    "\n",
    "def remove_multiple_spaces(string):\n",
    "\treturn re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "def save_in_pickle(save_object, save_file):\n",
    "\twith open(save_file, \"wb\") as pickle_out:\n",
    "\t\tpickle.dump(save_object, pickle_out)\n",
    "\n",
    "def load_from_pickle(pickle_file):\n",
    "\twith open(pickle_file, \"rb\") as pickle_in:\n",
    "\t\treturn pickle.load(pickle_in)\n",
    "\n",
    "def save_in_txt(list_of_strings, save_file):\n",
    "\twith open(save_file, \"w\") as writer:\n",
    "\t\tfor line in list_of_strings:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\twriter.write(f\"{line}\\n\")\n",
    "\n",
    "def load_from_txt(txt_file):\n",
    "\twith open(txt_file, \"r\") as reader:\n",
    "\t\tall_lines = list()\n",
    "\t\tfor line in reader:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tall_lines.append(line)\n",
    "\t\treturn all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c53d541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76ade05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'distilbert-base-uncased' \n",
    "# Bert Imports\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "#bert_model = DistilBertModel.from_pretrained(bert_model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8b13a",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e88d6",
   "metadata": {},
   "source": [
    "### Poetry Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9646d387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573\n",
      "                                    author  \\\n",
      "0                      WILLIAM SHAKESPEARE   \n",
      "1  DUCHESS OF NEWCASTLE MARGARET CAVENDISH   \n",
      "2                           THOMAS BASTARD   \n",
      "3                           EDMUND SPENSER   \n",
      "4                        RICHARD BARNFIELD   \n",
      "\n",
      "                                             content  \\\n",
      "0  Let the bird of loudest lay\\nOn the sole Arabi...   \n",
      "1  Sir Charles into my chamber coming in,\\nWhen I...   \n",
      "2  Our vice runs beyond all that old men saw,\\nAn...   \n",
      "3  Lo I the man, whose Muse whilome did maske,\\nA...   \n",
      "4  Long have I longd to see my love againe,\\nStil...   \n",
      "\n",
      "                                 poem name          age                  type  \n",
      "0               The Phoenix and the Turtle  Renaissance  Mythology & Folklore  \n",
      "1                 An Epilogue to the Above  Renaissance  Mythology & Folklore  \n",
      "2                       Book 7, Epigram 42  Renaissance  Mythology & Folklore  \n",
      "3  from The Faerie Queene: Book I, Canto I  Renaissance  Mythology & Folklore  \n",
      "4                                Sonnet 16  Renaissance  Mythology & Folklore  \n"
     ]
    }
   ],
   "source": [
    "data_file = '../data/with_epoque.csv'\n",
    "data = pd.read_csv(data_file)\n",
    "print(len(data))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd904746",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b14462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_training(df, char_max_line = 20):\n",
    "    inputs = []\n",
    "    context = []\n",
    "    targets = []\n",
    "    for i,rows in df.iterrows():\n",
    "        splitted = rows['content'].split('\\r\\n')\n",
    "        for line in splitted:\n",
    "            if len(line.strip()) > 0 and len(line.split(' ')) <= char_max_line:\n",
    "                inputs.append(line)\n",
    "                targets.append(line)\n",
    "                context.append(' '.join([str(rows['poem name'])]))\n",
    "        \n",
    "    return pd.DataFrame(list(zip(inputs, context, targets)),columns =['text', 'context','target'])\n",
    "\n",
    "\n",
    "#Defining torch dataset class for poems\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b1b10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_data_training(data, char_max_line = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6ff9fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the vocabulary = 319\n"
     ]
    }
   ],
   "source": [
    "pad_word = \"<pad>\"\n",
    "bos_word = \"<bos>\"\n",
    "eos_word = \"<eos>\"\n",
    "unk_word = \"<unk>\"\n",
    "sep_word = \"sep\"\n",
    "\n",
    "pad_id = 0\n",
    "bos_id = 1\n",
    "eos_id = 2\n",
    "unk_id = 3\n",
    "sep_id = 4\n",
    "    \n",
    "def normalize_sentence(s):\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word_to_id = {pad_word: pad_id, bos_word: bos_id, eos_word:eos_id, unk_word: unk_id, sep_word: sep_id}\n",
    "        self.word_count = {}\n",
    "        self.id_to_word = {pad_id: pad_word, bos_id: bos_word, eos_id: eos_word, unk_id: unk_word, sep_id: sep_word}\n",
    "        self.num_words = 5\n",
    "    \n",
    "    def get_ids_from_sentence(self, sentence):\n",
    "        sentence = normalize_sentence(sentence)\n",
    "        sent_ids = [bos_id] + [self.word_to_id[word.lower()] if word.lower() in self.word_to_id \\\n",
    "                               else unk_id for word in sentence.split()] + \\\n",
    "                               [eos_id]\n",
    "        return sent_ids\n",
    "    \n",
    "    def tokenized_sentence(self, sentence):\n",
    "        sent_ids = self.get_ids_from_sentence(sentence)\n",
    "        return [self.id_to_word[word_id] for word_id in sent_ids]\n",
    "\n",
    "    def decode_sentence_from_ids(self, sent_ids):\n",
    "        words = list()\n",
    "        for i, word_id in enumerate(sent_ids):\n",
    "            if word_id in [bos_id, eos_id, pad_id]:\n",
    "                # Skip these words\n",
    "                continue\n",
    "            else:\n",
    "                words.append(self.id_to_word[word_id])\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def add_words_from_sentence(self, sentence):\n",
    "        sentence = normalize_sentence(sentence)\n",
    "        for word in sentence.split():\n",
    "            if word not in self.word_to_id:\n",
    "                # add this word to the vocabulary\n",
    "                self.word_to_id[word] = self.num_words\n",
    "                self.id_to_word[self.num_words] = word\n",
    "                self.word_count[word] = 1\n",
    "                self.num_words += 1\n",
    "            else:\n",
    "                # update the word count\n",
    "                self.word_count[word] += 1\n",
    "\n",
    "vocab = Vocabulary()\n",
    "for src in df['text']:\n",
    "    vocab.add_words_from_sentence(src.lower())\n",
    "\n",
    "print(f\"Total words in the vocabulary = {vocab.num_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46f36e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poem_dataset(Dataset):\n",
    "    \"\"\"Single-Turn version of Cornell Movie Dialog Cropus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, poems, context,vocab, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            conversations: list of tuple (src_string, tgt_string) \n",
    "                         - src_string: String of the source sentence\n",
    "                         - tgt_string: String of the target sentence\n",
    "            vocab: Vocabulary object that contains the mapping of \n",
    "                    words to indices\n",
    "            device: cpu or cuda\n",
    "        \"\"\"\n",
    "        l = []\n",
    "        \n",
    "        for i in range(len(poems)):\n",
    "            l.append( ( context[i] + ' sep ' + poems[i] , poems[i] ))\n",
    "        \n",
    "        self.conversations = l.copy()\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "\n",
    "        def encode(src, tgt):\n",
    "            src_ids = self.vocab.get_ids_from_sentence(src)\n",
    "            tgt_ids = self.vocab.get_ids_from_sentence(tgt)\n",
    "            return (src_ids, tgt_ids)\n",
    "\n",
    "        # We will pre-tokenize the conversations and save in id lists for later use\n",
    "        self.tokenized_conversations = [encode(src, tgt) for src, tgt in self.conversations]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return {\"conv_ids\":self.tokenized_conversations[idx], \"conv\":self.conversations[idx]}\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (src_seq, tgt_seq).\n",
    "    We should build a custom collate_fn rather than using default collate_fn,\n",
    "    because merging sequences (including padding) is not supported in default.\n",
    "    Seqeuences are padded to the maximum length of mini-batch sequences (dynamic padding).\n",
    "    Args:\n",
    "        data: list of dicts {\"conv_ids\":(src_ids, tgt_ids), \"conv\":(src_str, trg_str)}.\n",
    "            - src_ids: list of src piece ids; variable length.\n",
    "            - tgt_ids: list of tgt piece ids; variable length.\n",
    "            - src_str: String of src\n",
    "            - tgt_str: String of tgt\n",
    "    Returns: dict { \"conv_ids\":     (src_ids, tgt_ids), \n",
    "                    \"conv\":         (src_str, tgt_str), \n",
    "                    \"conv_tensors\": (src_seqs, tgt_seqs)}\n",
    "            src_seqs: torch tensor of shape (src_padded_length, batch_size).\n",
    "            tgt_seqs: torch tensor of shape (tgt_padded_length, batch_size).\n",
    "            src_padded_length = length of the longest src sequence from src_ids\n",
    "            tgt_padded_length = length of the longest tgt sequence from tgt_ids\n",
    "    \"\"\"\n",
    "    # Sort conv_ids based on decreasing order of the src_lengths.\n",
    "    # This is required for efficient GPU computations.\n",
    "    src_ids = [torch.LongTensor(e[\"conv_ids\"][0]) for e in data]\n",
    "    tgt_ids = [torch.LongTensor(e[\"conv_ids\"][1]) for e in data]\n",
    "    src_str = [e[\"conv\"][0] for e in data]\n",
    "    tgt_str = [e[\"conv\"][1] for e in data]\n",
    "    data = list(zip(src_ids, tgt_ids, src_str, tgt_str))\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    src_ids, tgt_ids, src_str, tgt_str = zip(*data)\n",
    "\n",
    "\n",
    "    # Pad the src_ids and tgt_ids using token pad_id to create src_seqs and tgt_seqs\n",
    "    \n",
    "    # Implementation tip: You can use the nn.utils.rnn.pad_sequence utility\n",
    "    # function to combine a list of variable-length sequences with padding.\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    src_seqs = nn.utils.rnn.pad_sequence(src_ids, padding_value = pad_id,\n",
    "                                         batch_first = False)\n",
    "    tgt_seqs = nn.utils.rnn.pad_sequence(tgt_ids, padding_value = pad_id, \n",
    "                                         batch_first = False)\n",
    "    \n",
    "    src_padded_length = len(src_seqs[0])\n",
    "    tgt_padded_length = len(tgt_seqs[0])\n",
    "    return {\"conv_ids\":(src_ids, tgt_ids), \"conv\":(src_str, tgt_str), \"conv_tensors\":(src_seqs.to(device), tgt_seqs.to(device))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f56a887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for all_conversations\n",
    "\n",
    "all_poems = df['text'].tolist()\n",
    "context = df['context'].tolist()\n",
    "\n",
    "dataset = Poem_dataset(all_poems, context, vocab, device)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, \n",
    "                               shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba88c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written in her French Psalter sep No crooked leg, no bleared eye,\n",
      "No part deformed out of kind,\n",
      "Nor yet so ugly half can be\n",
      "As is the inward suspicious mind.\n",
      "['<bos>', '<unk>', 'in', 'her', '<unk>', '<unk>', 'sep', 'no', 'crooked', 'leg', 'no', 'bleared', 'eye', 'no', 'part', 'deformed', 'out', 'of', 'kind', 'nor', 'yet', 'so', 'ugly', 'half', 'can', 'be', 'as', 'is', 'the', 'inward', 'suspicious', 'mind', '.', '<eos>']\n",
      "[1, 3, 89, 232, 3, 3, 4, 5, 6, 7, 5, 8, 9, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 2]\n",
      "<unk> in her <unk> <unk> sep no crooked leg no bleared eye no part deformed out of kind nor yet so ugly half can be as is the inward suspicious mind .\n",
      "\n",
      "Song of the Witches: Double, double toil and trouble sep Notes:\n",
      "Macbeth: IV.i 10-19; 35-38\n",
      "['<bos>', 'song', 'of', 'the', '<unk>', '<unk>', '<unk>', '<unk>', 'and', '<unk>', 'sep', 'notes', 'macbeth', 'iv', '.i', '<eos>']\n",
      "[1, 281, 13, 24, 3, 3, 3, 3, 45, 3, 4, 29, 30, 31, 32, 2]\n",
      "song of the <unk> <unk> <unk> <unk> and <unk> sep notes macbeth iv .i\n",
      "\n",
      "Amoretti IV: \"New yeare forth looking out of Janus gate\" sep New yeare forth looking out of Janus gate,\n",
      "Doth seeme to promise hope of new delight:\n",
      "And bidding thold Adieu, his pass\n",
      "['<bos>', '<unk>', 'iv', 'new', 'yeare', 'forth', 'looking', 'out', 'of', 'janus', 'gate', 'sep', 'new', 'yeare', 'forth', 'looking', 'out', 'of', 'janus', 'gate', 'doth', 'seeme', 'to', 'promise', 'hope', 'of', 'new', 'delight', 'and', 'bidding', 'thold', 'adieu', 'his', 'pass', '<eos>']\n",
      "[1, 3, 31, 33, 34, 35, 36, 12, 13, 37, 38, 4, 33, 34, 35, 36, 12, 13, 37, 38, 39, 40, 41, 42, 43, 13, 33, 44, 45, 46, 47, 48, 49, 50, 2]\n",
      "<unk> iv new yeare forth looking out of janus gate sep new yeare forth looking out of janus gate doth seeme to promise hope of new delight and bidding thold adieu his pass\n",
      "\n",
      "Word = the\n",
      "Word ID = 24\n",
      "Word decoded from ID = the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for src, tgt in dataset.conversations[:3]:\n",
    "    sentence = src\n",
    "    word_tokens = vocab.tokenized_sentence(sentence)\n",
    "    # Automatically adds bos_id and eos_id before and after sentence ids respectively\n",
    "    word_ids = vocab.get_ids_from_sentence(sentence)\n",
    "    print(sentence)\n",
    "    print(word_tokens)\n",
    "    print(word_ids)\n",
    "    print(vocab.decode_sentence_from_ids(word_ids))\n",
    "    print()\n",
    "\n",
    "word = \"the\"\n",
    "word_id = vocab.word_to_id[word.lower()]\n",
    "print(f\"Word = {word}\")\n",
    "print(f\"Word ID = {word_id}\")\n",
    "print(f\"Word decoded from ID = {vocab.decode_sentence_from_ids([word_id])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76bfb435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing first training batch of size 5\n",
      "List of source strings:\n",
      "The Poem that Took the Place of a Mountain sep Wallace Stevens, \"The Poem that Took the Place of a Mountain\" from The Collected Poems. Copyright  1954 by Wallace Stevens.  Reprinted by permission of Random House, Inc.\n",
      "Ars Poetica sep Archibald MacLeish, Ars Poetica from Collected Poems 1917-1982. Copyright  1985 by The Estate of Archibald MacLeish. Reprinted with the permission of Houghton Mifflin Company. All rights reserved.\n",
      "Written in her French Psalter sep No crooked leg, no bleared eye,\n",
      "No part deformed out of kind,\n",
      "Nor yet so ugly half can be\n",
      "As is the inward suspicious mind.\n",
      "The Eemis Stane sep Hugh MacDiarmid, The Eemis Stane from Selected Poetry. Copyright  1992 by Alan Riach and Michael Grieve. Reprinted with the permission of New Directions Publishing Corporation.\n",
      "To a Dead Lover sep Originally published in Poetry, August 1922.\n",
      "\n",
      "Tokenized source ids:\n",
      "tensor([  1,  24, 109, 110, 111,  24, 112,  13,  90, 113,   4, 107, 108,  24,\n",
      "        109, 110, 111,  24, 112,  13,  90, 113,  55,  24, 114,  73,  28,  57,\n",
      "         58, 107, 108,  28,  59,  58,  60,  13, 115, 116, 117,  28,   2])\n",
      "tensor([  1, 227, 228,   4, 144, 145, 227, 228,  55, 114,  73,  28,  57,  58,\n",
      "         24, 147,  13, 144, 145,  28,  59,  92,  24,  60,  13, 148, 149, 150,\n",
      "         28,  84,  85,  86,  28,   2])\n",
      "tensor([  1,   3,  89, 232,   3,   3,   4,   5,   6,   7,   5,   8,   9,   5,\n",
      "         10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,\n",
      "         24,  25,  26,  27,  28,   2])\n",
      "tensor([  1,  24, 250, 251,   4,  99, 100,  24, 250, 251,  55,  94, 102,  28,\n",
      "         57,  58, 103, 104,  45, 105, 106,  28,  59,  92,  24,  60,  13,  33,\n",
      "         61,  62,  63,  28,   2])\n",
      "tensor([  1,  41,  90,   3, 242,   4, 198,  95,  89, 102, 311,  28,   2])\n",
      "\n",
      "Padded source ids as tensor (shape torch.Size([41, 5])):\n",
      "tensor([[  1,   1,   1,   1,   1],\n",
      "        [ 24, 227,   3,  24,  41],\n",
      "        [109, 228,  89, 250,  90],\n",
      "        [110,   4, 232, 251,   3],\n",
      "        [111, 144,   3,   4, 242],\n",
      "        [ 24, 145,   3,  99,   4],\n",
      "        [112, 227,   4, 100, 198],\n",
      "        [ 13, 228,   5,  24,  95],\n",
      "        [ 90,  55,   6, 250,  89],\n",
      "        [113, 114,   7, 251, 102],\n",
      "        [  4,  73,   5,  55, 311],\n",
      "        [107,  28,   8,  94,  28],\n",
      "        [108,  57,   9, 102,   2],\n",
      "        [ 24,  58,   5,  28,   0],\n",
      "        [109,  24,  10,  57,   0],\n",
      "        [110, 147,  11,  58,   0],\n",
      "        [111,  13,  12, 103,   0],\n",
      "        [ 24, 144,  13, 104,   0],\n",
      "        [112, 145,  14,  45,   0],\n",
      "        [ 13,  28,  15, 105,   0],\n",
      "        [ 90,  59,  16, 106,   0],\n",
      "        [113,  92,  17,  28,   0],\n",
      "        [ 55,  24,  18,  59,   0],\n",
      "        [ 24,  60,  19,  92,   0],\n",
      "        [114,  13,  20,  24,   0],\n",
      "        [ 73, 148,  21,  60,   0],\n",
      "        [ 28, 149,  22,  13,   0],\n",
      "        [ 57, 150,  23,  33,   0],\n",
      "        [ 58,  28,  24,  61,   0],\n",
      "        [107,  84,  25,  62,   0],\n",
      "        [108,  85,  26,  63,   0],\n",
      "        [ 28,  86,  27,  28,   0],\n",
      "        [ 59,  28,  28,   2,   0],\n",
      "        [ 58,   2,   2,   0,   0],\n",
      "        [ 60,   0,   0,   0,   0],\n",
      "        [ 13,   0,   0,   0,   0],\n",
      "        [115,   0,   0,   0,   0],\n",
      "        [116,   0,   0,   0,   0],\n",
      "        [117,   0,   0,   0,   0],\n",
      "        [ 28,   0,   0,   0,   0],\n",
      "        [  2,   0,   0,   0,   0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test one batch of training data\n",
    "first_batch = next(iter(data_loader))\n",
    "print(f\"Testing first training batch of size {len(first_batch['conv'][0])}\")\n",
    "print(f\"List of source strings:\")\n",
    "print_list(first_batch[\"conv\"][0])\n",
    "print(f\"Tokenized source ids:\")\n",
    "print_list(first_batch[\"conv_ids\"][0])\n",
    "print(f\"Padded source ids as tensor (shape {first_batch['conv_tensors'][0].size()}):\")\n",
    "print(first_batch[\"conv_tensors\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c344ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_collate_fn(batch, tokenizer):\n",
    "    bert_vocab = tokenizer.get_vocab()\n",
    "    bert_pad_token = bert_vocab['[PAD]']\n",
    "    bert_unk_token = bert_vocab['[UNK]']\n",
    "    bert_cls_token = bert_vocab['[CLS]']\n",
    "    inputs, masks_input, outputs, masks_output = [], [], [], []\n",
    "\n",
    "    sentences, masks_sentences, targets, masks_targets = [], [], [], []\n",
    "    for data in batch:\n",
    "\n",
    "        tokenizer_output = tokenizer([data['text']])\n",
    "        tokenized_sent = tokenizer_output['input_ids'][0]\n",
    "        \n",
    "        tokenizer_target = tokenizer([data['target']])\n",
    "        tokenized_sent_target = tokenizer_target['input_ids'][0]\n",
    "        \n",
    "        mask_sentence = tokenizer_output['attention_mask'][0]\n",
    "        mask_target = tokenizer_target['attention_mask'][0]\n",
    "        sentences.append(torch.tensor(tokenized_sent))\n",
    "        targets.append(torch.tensor(tokenized_sent_target))\n",
    "        masks_targets.append(torch.tensor(mask_targets))\n",
    "        masks_sentences.append(torch.tensor(mask_sentences))\n",
    "    sentences = pad_sequence(sentences, batch_first=True, padding_value=bert_pad_token)\n",
    "    targets = pad_sequence(targets, batch_first=True, padding_value=bert_pad_token)\n",
    "    masks = pad_sequence(masks, batch_first=True, padding_value=0.0)\n",
    "    return sentences, targets, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb74c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pytorch dataloaders from train_dataset, val_dataset, and test_datset\n",
    "batch_size=5\n",
    "train_dataloader = DataLoader(dataset,batch_size=batch_size,collate_fn=partial(transformer_collate_fn, tokenizer=tokenizer), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a70f00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.batch_decode(transformer_collate_fn(train_dataset,tokenizer)[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff4d18",
   "metadata": {},
   "source": [
    "## Polyencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b5c2271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "#bert1 = DistilBertModel.from_pretrained(bert_model_name)\n",
    "#bert2 = DistilBertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "bert = DistilBertModel.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21f509d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double Bert\n",
    "class RetrieverPolyencoder(nn.Module):\n",
    "    def __init__(self, contextBert, candidateBert, vocab, max_len = 300, hidden_dim = 768, out_dim = 64, num_layers = 2, dropout=0.1, device=device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_len = max_len\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        # Context layers\n",
    "        self.contextBert = contextBert\n",
    "        self.contextDropout = nn.Dropout(dropout)\n",
    "        self.contextFc = nn.Linear(self.hidden_dim, self.out_dim)\n",
    "        \n",
    "        # Candidates layers\n",
    "        self.candidatesBert = candidateBert\n",
    "        self.pos_emb = nn.Embedding(self.max_len, self.hidden_dim)\n",
    "        self.candidatesDropout = nn.Dropout(dropout)\n",
    "        self.candidatesFc = nn.Linear(self.hidden_dim, self.out_dim)\n",
    "        \n",
    "        self.att_dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def attention(self, q, k, v, vMask=None):\n",
    "        w = torch.matmul(q, k.transpose(-1, -2))\n",
    "        if vMask is not None:\n",
    "            w *= vMask.unsqueeze(1)\n",
    "            w = F.softmax(w, -1)\n",
    "        w = self.att_dropout(w)\n",
    "        score = torch.matmul(w, v)\n",
    "        return score\n",
    "\n",
    "    def score(self, context, context_mask, responses, responses_mask):\n",
    "        \"\"\"Run the model on the source and compute the loss on the target.\n",
    "\n",
    "        Args:\n",
    "            source: An integer tensor with shape (max_source_sequence_length,\n",
    "                batch_size) containing subword indices for the source sentences.\n",
    "            target: An integer tensor with shape (max_target_sequence_length,\n",
    "                batch_size) containing subword indices for the target sentences.\n",
    "\n",
    "        Returns:\n",
    "            A scalar float tensor representing cross-entropy loss on the current batch\n",
    "            divided by the number of target tokens in the batch.\n",
    "            Many of the target tokens will be pad tokens. You should mask the loss \n",
    "            from these tokens using appropriate mask on the target tokens loss.\n",
    "        \"\"\"\n",
    "        batch_size, nb_cand, seq_len = responses.shape\n",
    "        # Context\n",
    "        context_encoded = self.contextBert(context,context_mask)[-1]\n",
    "        pos_emb = self.pos_emb(torch.arange(self.max_len).to(self.device))\n",
    "        context_att = self.attention(pos_emb, context_encoded, context_encoded, context_mask)\n",
    "\n",
    "        # Response\n",
    "        responses_encoded = self.candidatesBert(responses.view(-1,responses.shape[2]), responses_mask.view(-1,responses.shape[2]))[-1][:,0,:]\n",
    "        responses_encoded = responses_encoded.view(batch_size,nb_cand,-1)\n",
    "        \n",
    "        context_emb = self.attention(responses_encoded, context_att, context_att).squeeze() \n",
    "        dot_product = (context_emb*responses_encoded).sum(-1)\n",
    "        \n",
    "        return dot_product\n",
    "\n",
    "    \n",
    "    def compute_loss(self, context, context_mask, response, response_mask):\n",
    "        \"\"\"Run the model on the source and compute the loss on the target.\n",
    "\n",
    "        Args:\n",
    "            source: An integer tensor with shape (max_source_sequence_length,\n",
    "                batch_size) containing subword indices for the source sentences.\n",
    "            target: An integer tensor with shape (max_target_sequence_length,\n",
    "                batch_size) containing subword indices for the target sentences.\n",
    "\n",
    "        Returns:\n",
    "            A scalar float tensor representing cross-entropy loss on the current batch\n",
    "            divided by the number of target tokens in the batch.\n",
    "            Many of the target tokens will be pad tokens. You should mask the loss \n",
    "            from these tokens using appropriate mask on the target tokens loss.\n",
    "        \"\"\"\n",
    "        batch_size = context.shape[0]\n",
    "        \n",
    "        # Context\n",
    "        context_encoded = self.contextBert(context,context_mask)[-1]\n",
    "        pos_emb = self.pos_emb(torch.arange(self.max_len).to(self.device))\n",
    "        context_att = self.attention(pos_emb, context_encoded, context_encoded, context_mask)\n",
    "\n",
    "        # Response\n",
    "        response_encoded = self.candidatesBert(response, response_mask)[-1][:,0,:]\n",
    "        \n",
    "        response_encoded = response_encoded.unsqueeze(0).expand(batch_size, batch_size, response_encoded.shape[1]) \n",
    "        context_emb = self.attention(response_encoded, context_att, context_att).squeeze() \n",
    "        dot_product = (context_emb*response_encoded).sum(-1)\n",
    "        mask = torch.eye(batch_size).to(self.device)\n",
    "        loss = F.log_softmax(dot_product, dim=-1) * mask\n",
    "        loss = (-loss.sum(dim=1)).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95b5ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Bert\n",
    "class RetrieverPolyencoder_single(nn.Module):\n",
    "    def __init__(self, bert, max_len = 300, hidden_dim = 768, out_dim = 64, num_layers = 2, dropout=0.1, device=device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_len = max_len\n",
    "        self.out_dim = out_dim\n",
    "        self.bert = bert\n",
    "        \n",
    "        # Context layers\n",
    "        self.contextDropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Candidates layers\n",
    "        self.pos_emb = nn.Embedding(self.max_len, self.hidden_dim)\n",
    "        self.candidatesDropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.att_dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def attention(self, q, k, v, vMask=None):\n",
    "        w = torch.matmul(q, k.transpose(-1, -2))\n",
    "        if vMask is not None:\n",
    "            w *= vMask.unsqueeze(1)\n",
    "            w = F.softmax(w, -1)\n",
    "        w = self.att_dropout(w)\n",
    "        score = torch.matmul(w, v)\n",
    "        return score\n",
    "\n",
    "    def score(self, context, context_mask, responses, responses_mask):\n",
    "        \"\"\"Run the model on the source and compute the loss on the target.\n",
    "\n",
    "        Args:\n",
    "            source: An integer tensor with shape (max_source_sequence_length,\n",
    "                batch_size) containing subword indices for the source sentences.\n",
    "            target: An integer tensor with shape (max_target_sequence_length,\n",
    "                batch_size) containing subword indices for the target sentences.\n",
    "\n",
    "        Returns:\n",
    "            A scalar float tensor representing cross-entropy loss on the current batch\n",
    "            divided by the number of target tokens in the batch.\n",
    "            Many of the target tokens will be pad tokens. You should mask the loss \n",
    "            from these tokens using appropriate mask on the target tokens loss.\n",
    "        \"\"\"\n",
    "        batch_size, nb_cand, seq_len = responses.shape\n",
    "        # Context\n",
    "        context_encoded = self.bert(context,context_mask)[0][:,0,:]\n",
    "        pos_emb = self.pos_emb(torch.arange(self.max_len).to(self.device))\n",
    "        context_att = self.attention(pos_emb, context_encoded, context_encoded, context_mask)\n",
    "\n",
    "        # Response\n",
    "        responses_encoded = self.bert(responses.view(-1,responses.shape[2]), responses_mask.view(-1,responses.shape[2]))[0][:,0,:]\n",
    "        responses_encoded = responses_encoded.view(batch_size,nb_cand,-1)\n",
    "        response_encoded = self.candidatesFc(response_encoded)\n",
    "        \n",
    "        context_emb = self.attention(responses_encoded, context_att, context_att).squeeze() \n",
    "        dot_product = (context_emb*responses_encoded).sum(-1)\n",
    "        \n",
    "        return dot_product\n",
    "\n",
    "    \n",
    "    def compute_loss(self, context, context_mask, response, response_mask):\n",
    "        \"\"\"Run the model on the source and compute the loss on the target.\n",
    "\n",
    "        Args:\n",
    "            source: An integer tensor with shape (max_source_sequence_length,\n",
    "                batch_size) containing subword indices for the source sentences.\n",
    "            target: An integer tensor with shape (max_target_sequence_length,\n",
    "                batch_size) containing subword indices for the target sentences.\n",
    "\n",
    "        Returns:\n",
    "            A scalar float tensor representing cross-entropy loss on the current batch\n",
    "            divided by the number of target tokens in the batch.\n",
    "            Many of the target tokens will be pad tokens. You should mask the loss \n",
    "            from these tokens using appropriate mask on the target tokens loss.\n",
    "        \"\"\"\n",
    "        batch_size = context.shape[0]\n",
    "        seq_len = response.shape[1]\n",
    "        \n",
    "        # Context\n",
    "        context_encoded = self.bert(context,context_mask)[0][:,0,:]\n",
    "        pos_emb = self.pos_emb(torch.arange(self.max_len).to(self.device))\n",
    "        context_att = self.attention(pos_emb, context_encoded, context_encoded, context_mask)\n",
    "\n",
    "        # Response\n",
    "        print(response.shape)\n",
    "        response_encoded = self.bert(response, response_mask)[0][:,0,:]\n",
    "        print(response_encoded.shape)\n",
    "        response_encoded = response_encoded.view(batch_size, -1)\n",
    "\n",
    "        \n",
    "        response_encoded = response_encoded.unsqueeze(0).expand(batch_size, batch_size, response_encoded.shape[1]) \n",
    "        context_emb = self.attention(response_encoded, context_att, context_att).squeeze() \n",
    "        dot_product = (context_emb*response_encoded).sum(-1)\n",
    "        mask = torch.eye(batch_size).to(self.device)\n",
    "        loss = F.log_softmax(dot_product, dim=-1) * mask\n",
    "        loss = (-loss.sum(dim=1)).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1bf5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bi-encoder\n",
    "class RetrieverBiencoder(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        \n",
    "    def score(self, context, context_mask, responses, responses_mask):\n",
    "\n",
    "        context_vec = self.bert(context, context_mask)[0][:,0,:]  # [bs,dim]\n",
    "\n",
    "        batch_size, res_length = response.shape\n",
    "\n",
    "        responses_vec = self.bert(responses_input_ids, responses_input_masks)[0][:,0,:]  # [bs,dim]\n",
    "        responses_vec = responses_vec.view(batch_size, 1, -1)\n",
    "\n",
    "        responses_vec = responses_vec.squeeze(1)        \n",
    "        context_vec = context_vec.unsqueeze(1)\n",
    "        dot_product = torch.matmul(context_vec, responses_vec.permute(0, 2, 1)).squeeze()\n",
    "        return dot_product\n",
    "    \n",
    "    def compute_loss(self, context, context_mask, response, response_mask):\n",
    "\n",
    "        context_vec = self.bert(context, context_mask)[0]  # [bs,dim]\n",
    "\n",
    "        batch_size, res_length = response.shape\n",
    "\n",
    "        responses_vec = self.bert(response, response_mask)[0][:,0,:]  # [bs,dim]\n",
    "        #responses_vec = responses_vec.view(batch_size, 1, -1)\n",
    "        \n",
    "        print(context_vec.shape)\n",
    "        print(responses_vec.shape)\n",
    "\n",
    "        responses_vec = responses_vec.squeeze(1)\n",
    "        dot_product = torch.matmul(context_vec, responses_vec.t())  # [bs, bs]\n",
    "        mask = torch.eye(context.size(0)).to(context_mask.device)\n",
    "        loss = F.log_softmax(dot_product, dim=-1) * mask\n",
    "        loss = (-loss.sum(dim=1)).mean()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0166378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs, model_file, learning_rate=0.0001):\n",
    "    \"\"\"Train the model for given µnumber of epochs and save the trained model in \n",
    "    the final model_file.\n",
    "    \"\"\"\n",
    "\n",
    "    decoder_learning_ratio = 5.0\n",
    "    #encoder_parameter_names = ['word_embedding', 'encoder']\n",
    "    encoder_parameter_names = ['encode_emb', 'encode_gru', 'l1', 'l2']\n",
    "                           \n",
    "    encoder_named_params = list(filter(lambda kv: any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
    "    decoder_named_params = list(filter(lambda kv: not any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
    "    encoder_params = [e[1] for e in encoder_named_params]\n",
    "    decoder_params = [e[1] for e in decoder_named_params]\n",
    "    optimizer = torch.optim.AdamW([{'params': encoder_params},\n",
    "                {'params': decoder_params, 'lr': learning_rate * decoder_learning_ratio}], lr=learning_rate)\n",
    "    \n",
    "    clip = 50.0\n",
    "    for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
    "        # print(f\"Total training instances = {len(train_dataset)}\")\n",
    "        # print(f\"train_data_loader = {len(train_data_loader)} {1180 > len(train_data_loader)/20}\")\n",
    "        with tqdm.notebook.tqdm(\n",
    "                data_loader,\n",
    "                desc=\"epoch {}\".format(epoch + 1),\n",
    "                unit=\"batch\",\n",
    "                total=len(data_loader)) as batch_iterator:\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for i, batch_data in enumerate(batch_iterator, start=1):\n",
    "                source, mask_source, target, mask_target = batch_data[\"conv_tensors\"]\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.compute_loss(source, mask_source, target, mask_target)\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                # Gradient clipping before taking the step\n",
    "                _ = nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_iterator.set_postfix(mean_loss=total_loss / i, current_loss=loss.item())\n",
    "    # Save the model after training         \n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75041d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d98654b84d442af8551333fac9a87a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe12ec59d404bbb9f4956c4b59ee2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 1:   0%|          | 0/15 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9579fbabad83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbaseline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetrieverBiencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"baseline_model.pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# Download the trained model to local for future use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#files.download('baseline_model.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-22afeb3c177c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, num_epochs, model_file, learning_rate)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conv_tensors\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-bdd2f59ae33a>\u001b[0m in \u001b[0;36mtransformer_collate_fn\u001b[0;34m(batch, tokenizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtokenizer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtokenized_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "# You are welcome to adjust these parameters based on your model implementation.\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "# Reloading the data_loader to increase batch_size\n",
    "\n",
    "baseline_model = RetrieverBiencoder(bert).to(device)\n",
    "train(baseline_model, train_dataloader, num_epochs, \"baseline_model.pt\",learning_rate=learning_rate)\n",
    "# Download the trained model to local for future use\n",
    "#files.download('baseline_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = RetrieverPolyencoder(bert1,bert2,vocab).to(device)\n",
    "baseline_model.load_state_dict(torch.load(\"baseline_model3.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781101cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = transformer_collate_fn(all_conversations[0:100],tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfa3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49fcd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = baseline_model.score(vals[0][i].unsqueeze(0).cuda(),vals[1][i].unsqueeze(0).cuda(),vals[2].unsqueeze(0).cuda(),vals[3].unsqueeze(0).cuda()).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb121a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conversations[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conversations[np.argmax(scores)][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bec300",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_v = 100\n",
    "vals = transformer_collate_fn(all_conversations[0:max_v],tokenizer)\n",
    "correct = 0\n",
    "for i in range(max_v):\n",
    "    scores = baseline_model.score(vals[0][i].unsqueeze(0).cuda(),vals[1][i].unsqueeze(0).cuda(),vals[2].unsqueeze(0).cuda(),vals[3].unsqueeze(0).cuda()).detach().cpu().numpy()\n",
    "    if np.argmax(scores)==i:\n",
    "        correct+=1\n",
    "    print(all_conversations[i][0])\n",
    "    print(all_conversations[np.argmax(scores)][1]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct/max_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5002a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
