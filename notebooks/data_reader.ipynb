{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002dbb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "import statistics\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import tqdm\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0cbfd4fe-0392-4388-a647-b379cfa2b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General util functions\n",
    "def make_dir_if_not_exists(directory):\n",
    "\tif not os.path.exists(directory):\n",
    "\t\tlogging.info(\"Creating new directory: {}\".format(directory))\n",
    "\t\tos.makedirs(directory)\n",
    "\n",
    "def print_list(l, K=None):\n",
    "\t# If K is given then only print first K\n",
    "\tfor i, e in enumerate(l):\n",
    "\t\tif i == K:\n",
    "\t\t\tbreak\n",
    "\t\tprint(e)\n",
    "\tprint()\n",
    "\n",
    "def remove_multiple_spaces(string):\n",
    "\treturn re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "def save_in_pickle(save_object, save_file):\n",
    "\twith open(save_file, \"wb\") as pickle_out:\n",
    "\t\tpickle.dump(save_object, pickle_out)\n",
    "\n",
    "def load_from_pickle(pickle_file):\n",
    "\twith open(pickle_file, \"rb\") as pickle_in:\n",
    "\t\treturn pickle.load(pickle_in)\n",
    "\n",
    "def save_in_txt(list_of_strings, save_file):\n",
    "\twith open(save_file, \"w\") as writer:\n",
    "\t\tfor line in list_of_strings:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\twriter.write(f\"{line}\\n\")\n",
    "\n",
    "def load_from_txt(txt_file):\n",
    "\twith open(txt_file, \"r\") as reader:\n",
    "\t\tall_lines = list()\n",
    "\t\tfor line in reader:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tall_lines.append(line)\n",
    "\t\treturn all_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d4909-a1d7-4587-a77b-ec875ee528df",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "268d5da7-4d1f-41e9-a906-f40f2f0a0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/'\n",
    "data_file = 'poem/poems.csv'\n",
    "dataset = pd.read_csv(data_folder + data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c532c-e48d-4705-a44b-1f279e4153d7",
   "metadata": {},
   "source": [
    "# Data vocabulary building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee219e29-27d9-4ec8-ac31-25069bd6b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_word = \"<pad>\"\n",
    "bos_word = \"<s>\"\n",
    "eos_word = \"</s>\"\n",
    "unk_word = \"<unk>\"\n",
    "pad_id = 0\n",
    "bos_id = 1\n",
    "eos_id = 2\n",
    "unk_id = 3\n",
    "    \n",
    "def normalize_sentence(s):\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word_to_id = {pad_word: pad_id, bos_word: bos_id, eos_word:eos_id, unk_word: unk_id}\n",
    "        self.word_count = {}\n",
    "        self.id_to_word = {pad_id: pad_word, bos_id: bos_word, eos_id: eos_word, unk_id: unk_word}\n",
    "        self.num_words = 4\n",
    "    \n",
    "    def get_ids_from_sentence(self, sentence):\n",
    "        sentence = normalize_sentence(sentence)\n",
    "        sent_ids = [bos_id] + [self.word_to_id[word] if word in self.word_to_id \\\n",
    "                               else unk_id for word in sentence.split()] + \\\n",
    "                               [eos_id]\n",
    "        return sent_ids\n",
    "    \n",
    "    def tokenized_sentence(self, sentence):\n",
    "        sent_ids = self.get_ids_from_sentence(sentence)\n",
    "        return [self.id_to_word[word_id] for word_id in sent_ids]\n",
    "\n",
    "    def decode_sentence_from_ids(self, sent_ids):\n",
    "        words = list()\n",
    "        for i, word_id in enumerate(sent_ids):\n",
    "            if word_id in [bos_id, eos_id, pad_id]:\n",
    "                # Skip these words\n",
    "                continue\n",
    "            else:\n",
    "                words.append(self.id_to_word[word_id])\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def add_words_from_sentence(self, sentence):\n",
    "        sentence = normalize_sentence(sentence)\n",
    "        for word in sentence.split():\n",
    "            if word not in self.word_to_id:\n",
    "                # add this word to the vocabulary\n",
    "                self.word_to_id[word] = self.num_words\n",
    "                self.id_to_word[self.num_words] = word\n",
    "                self.word_count[word] = 1\n",
    "                self.num_words += 1\n",
    "            else:\n",
    "                # update the word count\n",
    "                self.word_count[word] += 1\n",
    "\n",
    "vocab_data = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "db4fdc3b-5380-4da1-8290-3e76b5bfb1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/'\n",
    "data_file = 'PoetryFound.csv'\n",
    "dataset = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eb733fce-1f64-41d0-9eb9-b20e7d464c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(dataset, separator = '<SEP>', mod = 'line', i_max = 20000):\n",
    "    x, y = [], []\n",
    "    if mod == 'line':\n",
    "        for i, rows in dataset.iterrows():\n",
    "            poem = eval(rows['Poem'])\n",
    "            for poem_line in poem:\n",
    "                x.append( \"<s> \" + poem_line)\n",
    "                y.append(poem_line + \" </s>\")\n",
    "    if mod == 'poem':\n",
    "        for i, rows in dataset.iterrows():\n",
    "            poem = eval(rows['Poem'])\n",
    "            x.append( \"<s> \" + ' </s> '.join(poem))\n",
    "            y.append(' </s> '.join(poem) + \" </s>\")\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99abe53c-df64-4f06-88df-6c29d4522299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12899\n"
     ]
    }
   ],
   "source": [
    "x, y = generate_training_data(dataset, mod = 'poem')\n",
    "\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ff96b95-acfe-4c97-9cfb-8088851c2090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After Octavio Paz', 'Whatâ€™s most human must drive', 'an arrow to the heart.', 'Ghosts, too, must abide by this directive', '& remain transparent,', 'going about their business in old houses.', 'Before I was an I, I longed to be ethereal.', 'Sprouting wings at will & gliding through', 'cul-de-sacs and malls around the valley.', 'My hands, too, would gradually disappear', 'followed by my arms, then neck & head', 'until my whole body was slight as allergen.', 'Before I was an I, I spoke an old language', 'that would return on drowsy afternoons.', 'Therefore I struggled to say', 'the simplest sentences. So much so', 'that the maligned semicolon', 'became an ardent ally, an island', 'of pause and the deep breath.', 'The comma, too, bless its tiny soul,', 'was the crumb which the god', 'of small favors multiplied', 'tenfold for my morning pie.', 'Before I was an I, knowledge', 'clung to me like burrs & hunger', 'guided my ship like the barefoot light                on the sleeping land & sea.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.iloc()[11]['Poem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a5f5c-6968-4fc0-832e-1f849cd2a2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
