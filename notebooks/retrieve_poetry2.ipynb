{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbf34a7-a58e-49a2-aa4b-8bca5f58c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "import statistics\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4150bff-cdea-4d5e-984b-9e0606535672",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'distilbert-base-uncased' \n",
    "# Bert Imports\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "#bert_model = DistilBertModel.from_pretrained(bert_model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a1af93-f657-47ef-b5e3-3582a2d00c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.path.dirname(sys.path[0]),'src'))\n",
    "import retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36cedf6f-e5e8-4d3f-8d91-7e3d843b11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dir_if_not_exists(directory):\n",
    "\tif not os.path.exists(directory):\n",
    "\t\tlogging.info(\"Creating new directory: {}\".format(directory))\n",
    "\t\tos.makedirs(directory)\n",
    "\n",
    "def print_list(l, K=None):\n",
    "\tfor i, e in enumerate(l):\n",
    "\t\tif i == K:\n",
    "\t\t\tbreak\n",
    "\t\tprint(e)\n",
    "\tprint()\n",
    "\n",
    "def remove_multiple_spaces(string):\n",
    "\treturn re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "def save_in_pickle(save_object, save_file):\n",
    "\twith open(save_file, \"wb\") as pickle_out:\n",
    "\t\tpickle.dump(save_object, pickle_out)\n",
    "\n",
    "def load_from_pickle(pickle_file):\n",
    "\twith open(pickle_file, \"rb\") as pickle_in:\n",
    "\t\treturn pickle.load(pickle_in)\n",
    "\n",
    "def save_in_txt(list_of_strings, save_file):\n",
    "\twith open(save_file, \"w\") as writer:\n",
    "\t\tfor line in list_of_strings:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\twriter.write(f\"{line}\\n\")\n",
    "\n",
    "def load_from_txt(txt_file):\n",
    "\twith open(txt_file, \"r\") as reader:\n",
    "\t\tall_lines = list()\n",
    "\t\tfor line in reader:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tall_lines.append(line)\n",
    "\t\treturn all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88ed704e-8f0c-4750-8c2c-a085911ea7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20b3775-ea62-4292-97b9-c7f2e3d015a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573\n",
      "                                    author  \\\n",
      "0                      WILLIAM SHAKESPEARE   \n",
      "1  DUCHESS OF NEWCASTLE MARGARET CAVENDISH   \n",
      "2                           THOMAS BASTARD   \n",
      "3                           EDMUND SPENSER   \n",
      "4                        RICHARD BARNFIELD   \n",
      "\n",
      "                                             content  \\\n",
      "0  Let the bird of loudest lay\\nOn the sole Arabi...   \n",
      "1  Sir Charles into my chamber coming in,\\nWhen I...   \n",
      "2  Our vice runs beyond all that old men saw,\\nAn...   \n",
      "3  Lo I the man, whose Muse whilome did maske,\\nA...   \n",
      "4  Long have I longd to see my love againe,\\nStil...   \n",
      "\n",
      "                                 poem name          age                  type  \n",
      "0               The Phoenix and the Turtle  Renaissance  Mythology & Folklore  \n",
      "1                 An Epilogue to the Above  Renaissance  Mythology & Folklore  \n",
      "2                       Book 7, Epigram 42  Renaissance  Mythology & Folklore  \n",
      "3  from The Faerie Queene: Book I, Canto I  Renaissance  Mythology & Folklore  \n",
      "4                                Sonnet 16  Renaissance  Mythology & Folklore  \n"
     ]
    }
   ],
   "source": [
    "data_file = '../data/with_epoque.csv'\n",
    "data = pd.read_csv(data_file)\n",
    "print(len(data))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc84c964-5408-4e74-8e93-5aa1f62262eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_training(df, char_max_line = 20):\n",
    "    inputs = []\n",
    "    context = []\n",
    "    targets = []\n",
    "    previous = []\n",
    "    for i,rows in df.iterrows():\n",
    "        splitted = rows['content'].split('\\n')\n",
    "        if len(splitted) > 4:\n",
    "            for i,line in enumerate(splitted): \n",
    "                if len(line.strip()) > 0 and len(line.split(' ')) <= char_max_line:\n",
    "                    if i==0:\n",
    "                        previous.append(' ')\n",
    "                    else:\n",
    "                        previous.append(splitted[i-1])\n",
    "                    inputs.append(line)\n",
    "                    targets.append(line)\n",
    "                    context.append(' '.join([str(rows['author'])]))\n",
    "        \n",
    "    return pd.DataFrame(list(zip(inputs, context, targets, previous)),columns =['text', 'context','target', 'previous'])\n",
    "\n",
    "\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a8aecb5-c164-4b77-8849-4c686c2abb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    text              context  \\\n",
      "0            Let the bird of loudest lay  WILLIAM SHAKESPEARE   \n",
      "1               On the sole Arabian tree  WILLIAM SHAKESPEARE   \n",
      "2             Herald sad and trumpet be,  WILLIAM SHAKESPEARE   \n",
      "3      To whose sound chaste wings obey.  WILLIAM SHAKESPEARE   \n",
      "4          But thou shrieking harbinger,  WILLIAM SHAKESPEARE   \n",
      "...                                  ...                  ...   \n",
      "13480              And the lisp of reeds    RICHARD ALDINGTON   \n",
      "13481      And the sun upon thy breasts,    RICHARD ALDINGTON   \n",
      "13482           And thou hearest me not,    RICHARD ALDINGTON   \n",
      "13483                     Potuia, potuia    RICHARD ALDINGTON   \n",
      "13484               Thou hearest me not.    RICHARD ALDINGTON   \n",
      "\n",
      "                                  target                       previous  \n",
      "0            Let the bird of loudest lay                                 \n",
      "1               On the sole Arabian tree    Let the bird of loudest lay  \n",
      "2             Herald sad and trumpet be,       On the sole Arabian tree  \n",
      "3      To whose sound chaste wings obey.     Herald sad and trumpet be,  \n",
      "4          But thou shrieking harbinger,                                 \n",
      "...                                  ...                            ...  \n",
      "13480              And the lisp of reeds  I have told thee of the hills  \n",
      "13481      And the sun upon thy breasts,          And the lisp of reeds  \n",
      "13482           And thou hearest me not,                                 \n",
      "13483                     Potuia, potuia       And thou hearest me not,  \n",
      "13484               Thou hearest me not.                 Potuia, potuia  \n",
      "\n",
      "[13485 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df = make_data_training(data, char_max_line = 30)\n",
    "\n",
    "all_poems = df['text'].tolist()\n",
    "context = df['context'].tolist()\n",
    "previous = df['previous'].tolist()\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed3c5ad2-9551-4656-a1dc-897a2728f74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird lay let\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfIdfVectorizer=TfidfVectorizer()\n",
    "tfIdf = tfIdfVectorizer.fit_transform(all_poems)\n",
    "\n",
    "X = tfIdfVectorizer.transform([\"Let the bird of loudest lay\"])\n",
    "names = np.array(tfIdfVectorizer.get_feature_names())\n",
    "ind = np.array(X.indices[X.data.sort()][0][-3:][::-1])\n",
    "res = names[ind]\n",
    "\n",
    "print(' '.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4393a4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    text              context  \\\n",
      "0            Let the bird of loudest lay  WILLIAM SHAKESPEARE   \n",
      "1               On the sole Arabian tree  WILLIAM SHAKESPEARE   \n",
      "2             Herald sad and trumpet be,  WILLIAM SHAKESPEARE   \n",
      "3      To whose sound chaste wings obey.  WILLIAM SHAKESPEARE   \n",
      "4          But thou shrieking harbinger,  WILLIAM SHAKESPEARE   \n",
      "...                                  ...                  ...   \n",
      "13480              And the lisp of reeds    RICHARD ALDINGTON   \n",
      "13481      And the sun upon thy breasts,    RICHARD ALDINGTON   \n",
      "13482           And thou hearest me not,    RICHARD ALDINGTON   \n",
      "13483                     Potuia, potuia    RICHARD ALDINGTON   \n",
      "13484               Thou hearest me not.    RICHARD ALDINGTON   \n",
      "\n",
      "                                  target                       previous  \n",
      "0            Let the bird of loudest lay                                 \n",
      "1               On the sole Arabian tree    Let the bird of loudest lay  \n",
      "2             Herald sad and trumpet be,       On the sole Arabian tree  \n",
      "3      To whose sound chaste wings obey.     Herald sad and trumpet be,  \n",
      "4          But thou shrieking harbinger,                                 \n",
      "...                                  ...                            ...  \n",
      "13480              And the lisp of reeds  I have told thee of the hills  \n",
      "13481      And the sun upon thy breasts,          And the lisp of reeds  \n",
      "13482           And thou hearest me not,                                 \n",
      "13483                     Potuia, potuia       And thou hearest me not,  \n",
      "13484               Thou hearest me not.                 Potuia, potuia  \n",
      "\n",
      "[13485 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df = make_data_training(data, char_max_line = 30)\n",
    "\n",
    "all_poems = df['text'].tolist()\n",
    "context = df['context'].tolist()\n",
    "previous = df['previous'].tolist()\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e756d896-3ee8-474f-ade6-598bfdedf6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7592"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()[\"hello\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62e57386-ac09-497e-9040-b22699b38e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def normalize_sentence(s):\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def get_ids_from_sentence(self, sentence):\n",
    "        sentence = normalize_sentence(sentence)\n",
    "        sent_ids = self.tokenizer(sentence)\n",
    "        return sent_ids\n",
    "    \n",
    "    def tokenized_sentence(self, sentence):\n",
    "        sent_ids = self.get_ids_from_sentence(sentence)\n",
    "        return [self.tokenizer.decode(word_id) for word_id in sent_ids]\n",
    "    \n",
    "    def tokenized_sentence(self, sentence):\n",
    "        sent_ids = self.get_ids_from_sentence(sentence)\n",
    "        return tokenizer.decode(sent_ids, skip_special_tokens=True)\n",
    "\n",
    "    def decode_sentence_from_ids(self, sent_ids):\n",
    "        return \n",
    "\n",
    "vocab = Vocabulary(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca140986-dc0a-446c-8ac4-c40832df65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poem_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, poems, context, previous, vocab, device):\n",
    "\n",
    "        l = []\n",
    "        \n",
    "        for i in range(len(poems)):\n",
    "            X = tfIdfVectorizer.transform([poems[i]])\n",
    "            ind = np.array(X.indices[X.data.sort()][0][-3:][::-1])\n",
    "            key_words = names[ind]\n",
    "            l.append( (context[i] + \" sep \" + ' '.join(key_words), poems[i] ))\n",
    "        \n",
    "        self.poems = l.copy()\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "\n",
    "        def encode(src, tgt):\n",
    "            src_ids = self.vocab.get_ids_from_sentence(src)\n",
    "            tgt_ids = self.vocab.get_ids_from_sentence(tgt)\n",
    "            return (src_ids, tgt_ids)\n",
    "\n",
    "        # We will pre-tokenize the conversations and save in id lists for later use\n",
    "        self.tokenized_poems = [encode(src, tgt) for src, tgt in self.poems]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return {\"conv_ids\":self.tokenized_poems[idx], \"conv\":self.poems[idx]}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, masks_input, outputs, masks_output = [], [], [], []\n",
    "\n",
    "    for data in batch:\n",
    "\n",
    "        tokenizer_output = data['conv_ids'][0]\n",
    "        tokenized_sent = tokenizer_output['input_ids']\n",
    "        \n",
    "        tokenizer_target = data['conv_ids'][1]\n",
    "        tokenized_sent_target = tokenizer_target['input_ids']\n",
    "        \n",
    "        mask_sentence = tokenizer_output['attention_mask']\n",
    "        mask_target = tokenizer_target['attention_mask']\n",
    "        \n",
    "        inputs.append(torch.tensor(tokenized_sent).to(device))\n",
    "        outputs.append(torch.tensor(tokenized_sent_target).to(device))\n",
    "        masks_input.append(torch.tensor(mask_sentence).to(device))\n",
    "        masks_output.append(torch.tensor(mask_target).to(device))\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    outputs = pad_sequence(outputs, batch_first=True, padding_value=0)\n",
    "    masks_input = pad_sequence(masks_input, batch_first=True, padding_value=0.0)\n",
    "    masks_output = pad_sequence(masks_output, batch_first=True, padding_value=0.0)\n",
    "    return inputs, masks_input, outputs, masks_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be2f4894-e9ab-4874-bb2d-ff7ab45d9292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 102, 0, 101, 103]\n",
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.tokenizer.all_special_ids)\n",
    "print(vocab.tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a2568c4-1691-438d-8032-4efce1c000b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Poem_dataset(all_poems, context, previous, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84b29501-e2a3-4f45-a3e9-8287376d0326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for src, tgt in dataset.poems[0:5]:\\n    sentence = src\\n    word_tokens = vocab.tokenized_sentence(sentence)\\n    word_ids = vocab.get_ids_from_sentence(sentence)\\n    print(sentence)\\n    print(tgt)\\n    print(word_tokens)\\n    print()\\n\\nword = \"the\"\\nword_id = vocab.tokenizer(word.lower(),add_special_tokens=False)\\nprint(f\"Word = {word}\")\\nprint(f\"Word ID = {word_id}\")\\nprint(f\"Word decoded from ID = {vocab.decode_sentence_from_ids([word_id])}\")'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for src, tgt in dataset.poems[0:5]:\n",
    "    sentence = src\n",
    "    word_tokens = vocab.tokenized_sentence(sentence)\n",
    "    word_ids = vocab.get_ids_from_sentence(sentence)\n",
    "    print(sentence)\n",
    "    print(tgt)\n",
    "    print(word_tokens)\n",
    "    print()\n",
    "\n",
    "word = \"the\"\n",
    "word_id = vocab.tokenizer(word.lower(),add_special_tokens=False)\n",
    "print(f\"Word = {word}\")\n",
    "print(f\"Word ID = {word_id}\")\n",
    "print(f\"Word decoded from ID = {vocab.decode_sentence_from_ids([word_id])}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd5c86bc-fb94-4706-a149-41eab9370f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_GloVe(filename):\n",
    "  embeddings = {}\n",
    "  for line in open(filename).readlines():\n",
    "    fields = line.strip().split(\" \")\n",
    "    word = fields[0]\n",
    "    embeddings[word] = [float(x) for x in fields[1:]]\n",
    "  return embeddings\n",
    "\n",
    "GloVe = read_GloVe(\"../data/glove.840B.300d.conll_filtered.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c01483e-0a77-43d9-b785-90301515be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs, model_file, learning_rate=0.0001):\n",
    "\n",
    "    decoder_learning_ratio = 5.0\n",
    "    encoder_parameter_names = ['encode_emb', 'encode_gru', 'l1', 'l2']\n",
    "                           \n",
    "    encoder_named_params = list(filter(lambda kv: any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
    "    decoder_named_params = list(filter(lambda kv: not any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
    "    encoder_params = [e[1] for e in encoder_named_params]\n",
    "    decoder_params = [e[1] for e in decoder_named_params]\n",
    "    optimizer = torch.optim.AdamW([{'params': encoder_params},\n",
    "                {'params': decoder_params, 'lr': learning_rate * decoder_learning_ratio}], lr=learning_rate)\n",
    "    \n",
    "    clip = 50.0\n",
    "    for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
    "        with tqdm.notebook.tqdm(\n",
    "                data_loader,\n",
    "                desc=\"epoch {}\".format(epoch + 1),\n",
    "                unit=\"batch\",\n",
    "                total=len(data_loader)) as batch_iterator:\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for i, batch_data in enumerate(batch_iterator, start=1):\n",
    "                source, source_mask, target, target_mask = batch_data\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.compute_loss(source, source_mask, target, target_mask)\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                _ = nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_iterator.set_postfix(mean_loss=total_loss / i, current_loss=loss.item())\n",
    "       \n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56066f16-c0f0-4c45-8ac2-53d0f918695e",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65e9247c-de19-4c2c-bd98-6aa57ca74bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = DistilBertModel.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55260cf1-8ca4-4de1-af19-31d0b89de0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model = retrievers.RetrieverPolyencoder(bert,device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3250c3a-5cd5-45df-99ee-bfa97cdc34a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23692217e254f91949e06c7bccf46dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/5 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd89ca13d424771b0ef5ed64350095b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 1:   0%|          | 0/211 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e67a591b4fb4c4d8b6f7953fa9d3a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 2:   0%|          | 0/211 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c393f5241e49aa814ba442d04283e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 3:   0%|          | 0/211 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3642feeeb694d5b8cbe2179011268ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 4:   0%|          | 0/211 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 4.00 GiB total capacity; 2.28 GiB already allocated; 0 bytes free; 2.48 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4746/2673120799.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretriever_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"baseline_model.pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4746/3253928793.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, num_epochs, model_file, learning_rate)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs7643-a4/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs7643-a4/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 4.00 GiB total capacity; 2.28 GiB already allocated; 0 bytes free; 2.48 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, \n",
    "                               shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "train(retriever_model, data_loader, num_epochs, \"baseline_model.pt\",learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d37ef-ac4f-4791-9ad5-3b350f514517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_greedy(model, sentence, max_length=100):\n",
    "    \"\"\"Make predictions for the given input using greedy inference.\n",
    "    \n",
    "    Args:\n",
    "        model: A sequence-to-sequence model.\n",
    "        sentence: A input string.\n",
    "        max_length: The maximum length at which to truncate outputs in order to\n",
    "            avoid non-terminating inference.\n",
    "    \n",
    "    Returns:\n",
    "        Model's predicted greedy response for the input, represented as string.\n",
    "    \"\"\"\n",
    "\n",
    "    # You should make only one call to model.encode() at the start of the function, \n",
    "    # and make only one call to model.decode() per inference step.\n",
    "    model.eval()    \n",
    "    src_id = torch.tensor(vocab.get_ids_from_sentence(sentence))[:,None].to(device)\n",
    "    encoder_output, encoder_mask, last_hidden = model.encode(src_id) \n",
    "    input = src_id[0,:]\n",
    "    out = [bos_id]\n",
    "    for t in range(max_length):\n",
    "        input = input[None,:]\n",
    "        out_decoder, last_hidden, _ = model.decode(input, last_hidden, encoder_output, encoder_mask)\n",
    "        input = out_decoder.argmax(dim=-1)\n",
    "        word = input.item()\n",
    "        out.append(word)\n",
    "        if word == eos_id:\n",
    "            break\n",
    "    \n",
    "    decoded = vocab.decode_sentence_from_ids(out)\n",
    "    return decoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a35318-02d0-46b1-9be2-e57f82b556e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_beam(model, sentence, k=5, max_length=100, hidden = None):\n",
    "\n",
    "    alpha = 0.3\n",
    "    model.eval()\n",
    "    \n",
    "    sentence_ids = torch.tensor(vocab.get_ids_from_sentence(sentence)).cuda()\n",
    "    sentence_ids = sentence_ids.unsqueeze(1)\n",
    "    encoder_output, encoder_mask, h = model.encode(sentence_ids)\n",
    "\n",
    "    out_start = sentence_ids[0]\n",
    "    beam = [out_start for i in range(k)]\n",
    "    beam_scores = [1 for i in range(k)]\n",
    "    \n",
    "    if hidden:\n",
    "        h = hidden\n",
    "    hiddens = [h for i in range(k)]\n",
    "    \n",
    "    generations = []\n",
    "    generations_scores = []\n",
    "    curr_l = 0\n",
    "    eos_tensor = torch.Tensor([eos_id]).int().cuda()\n",
    "    while beam:\n",
    "        logits = torch.Tensor().cuda()\n",
    "        inds = torch.Tensor().int().cuda()\n",
    "        curr_k = len(beam)\n",
    "        if curr_l==max_length:\n",
    "            for i in range(curr_k):\n",
    "                  generations += [torch.cat((beam[i],eos_tensor),0)]\n",
    "                  generations_scores += [new_beam_scores[i]]\n",
    "            break\n",
    "        else:\n",
    "            for i in range(curr_k):\n",
    "                out, hiddens[i], _ = model.decode(beam[i][-1].view(1,1), hiddens[i], encoder_output,\n",
    "                                     encoder_mask)\n",
    "                logit,ind = torch.topk(out.squeeze(), curr_k, dim=0)\n",
    "                logits = torch.cat((logits,logit),0)\n",
    "                inds = torch.cat((inds,ind),0)\n",
    "            new_beam = []\n",
    "            new_beam_scores = []\n",
    "            new_hiddens = []\n",
    "            if curr_l==0:\n",
    "                for i in range(curr_k):\n",
    "                    max_ind = torch.argmax(nn.functional.log_softmax(logit,dim=0))\n",
    "                    new_beam_scores += [float(logit[max_ind])]\n",
    "                    logit[max_ind] = -1e9\n",
    "                    new_beam += [torch.cat((beam[0],ind[max_ind].unsqueeze(0)),0)]\n",
    "                    new_hiddens += [hiddens[0]]\n",
    "            else:\n",
    "                top_logits,top_inds_logit = torch.topk(torch.repeat_interleave(torch.Tensor(beam_scores).cuda(),\n",
    "                                                                               curr_k)\\\n",
    "                                                       +nn.functional.log_softmax(logits,dim=0),\n",
    "                                                       curr_k, dim=0)\n",
    "                for i in range(curr_k):\n",
    "                    if inds[top_inds_logit[i]]==eos_id:\n",
    "                        generations += [torch.cat((beam[top_inds_logit[i]//curr_k],inds[top_inds_logit[i]].unsqueeze(0)),0)]\n",
    "                        generations_scores+=[float(logits[top_inds_logit[i]])/(generations[-1].shape[0]**alpha)]\n",
    "                    else:\n",
    "                        new_beam += [torch.cat((beam[top_inds_logit[i]//curr_k],inds[top_inds_logit[i]].unsqueeze(0)),0)]\n",
    "                        new_hiddens += [hiddens[top_inds_logit[i]//curr_k]]\n",
    "                        new_beam_scores += [float(logits[top_inds_logit[i]])]\n",
    "            beam = new_beam\n",
    "            beam_scores = new_beam_scores\n",
    "            hiddens = new_hiddens\n",
    "        curr_l +=1\n",
    "    generations = [g for _, g in sorted(zip(generations_scores, generations))]\n",
    "    generations.reverse()\n",
    "    return [vocab.decode_sentence_from_ids(s.tolist()) for s in generations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8d71f-84c9-4cab-acf9-17b9d1b3b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"WILLIAM SHAKESPEARE sep love moon bride sep tell me the love of comrades sweetens !\"\n",
    "print(predict_greedy(Erato_model, sentence, max_length=100))\n",
    "print()\n",
    "predict_beam(Erato_model, sentence, k=60, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b7a81-5122-4228-bda7-41a7167fbcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfbb248-3d18-46f7-bfc6-e2c180a0d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "p = data.iloc()[i]\n",
    "\n",
    "print(p['author'])\n",
    "print(p['poem name'])\n",
    "print(p['age'])\n",
    "print()\n",
    "print(p['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca76af-bcd4-4806-988a-0caf3148987a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
