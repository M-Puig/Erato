{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbf34a7-a58e-49a2-aa4b-8bca5f58c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "import statistics\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import tqdm\n",
    "import nltk\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cedf6f-e5e8-4d3f-8d91-7e3d843b11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General util functions\n",
    "def make_dir_if_not_exists(directory):\n",
    "\tif not os.path.exists(directory):\n",
    "\t\tlogging.info(\"Creating new directory: {}\".format(directory))\n",
    "\t\tos.makedirs(directory)\n",
    "\n",
    "def print_list(l, K=None):\n",
    "\t# If K is given then only print first K\n",
    "\tfor i, e in enumerate(l):\n",
    "\t\tif i == K:\n",
    "\t\t\tbreak\n",
    "\t\tprint(e)\n",
    "\tprint()\n",
    "\n",
    "def remove_multiple_spaces(string):\n",
    "\treturn re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "def save_in_pickle(save_object, save_file):\n",
    "\twith open(save_file, \"wb\") as pickle_out:\n",
    "\t\tpickle.dump(save_object, pickle_out)\n",
    "\n",
    "def load_from_pickle(pickle_file):\n",
    "\twith open(pickle_file, \"rb\") as pickle_in:\n",
    "\t\treturn pickle.load(pickle_in)\n",
    "\n",
    "def save_in_txt(list_of_strings, save_file):\n",
    "\twith open(save_file, \"w\") as writer:\n",
    "\t\tfor line in list_of_strings:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\twriter.write(f\"{line}\\n\")\n",
    "\n",
    "def load_from_txt(txt_file):\n",
    "\twith open(txt_file, \"r\") as reader:\n",
    "\t\tall_lines = list()\n",
    "\t\tfor line in reader:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tall_lines.append(line)\n",
    "\t\treturn all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ed704e-8f0c-4750-8c2c-a085911ea7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20b3775-ea62-4292-97b9-c7f2e3d015a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573\n",
      "                                    author  \\\n",
      "0                      WILLIAM SHAKESPEARE   \n",
      "1  DUCHESS OF NEWCASTLE MARGARET CAVENDISH   \n",
      "2                           THOMAS BASTARD   \n",
      "3                           EDMUND SPENSER   \n",
      "4                        RICHARD BARNFIELD   \n",
      "\n",
      "                                             content  \\\n",
      "0  Let the bird of loudest lay\\r\\nOn the sole Ara...   \n",
      "1  Sir Charles into my chamber coming in,\\r\\nWhen...   \n",
      "2  Our vice runs beyond all that old men saw,\\r\\n...   \n",
      "3  Lo I the man, whose Muse whilome did maske,\\r\\...   \n",
      "4  Long have I longd to see my love againe,\\r\\nSt...   \n",
      "\n",
      "                                 poem name          age                  type  \n",
      "0               The Phoenix and the Turtle  Renaissance  Mythology & Folklore  \n",
      "1                 An Epilogue to the Above  Renaissance  Mythology & Folklore  \n",
      "2                       Book 7, Epigram 42  Renaissance  Mythology & Folklore  \n",
      "3  from The Faerie Queene: Book I, Canto I  Renaissance  Mythology & Folklore  \n",
      "4                                Sonnet 16  Renaissance  Mythology & Folklore  \n"
     ]
    }
   ],
   "source": [
    "data_file = 'with_epoque.csv'\n",
    "data = pd.read_csv(data_file)\n",
    "print(len(data))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8073de9-4b6d-4117-8704-60a2266e4532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_training(df, char_max_line = 20):\n",
    "    inputs = []\n",
    "    context = []\n",
    "    targets = []\n",
    "    for i,rows in df.iterrows():\n",
    "        splitted = rows['content'].split('\\r\\n')\n",
    "        for line in splitted:\n",
    "            if len(line.strip()) > 0 and len(line.split(' ')) <= char_max_line:\n",
    "                inputs.append(line)\n",
    "                targets.append(line)\n",
    "                context.append(' '.join([str(rows['poem name'])]))\n",
    "        \n",
    "    return pd.DataFrame(list(zip(inputs, context, targets)),columns =['text', 'context','target'])\n",
    "\n",
    "\n",
    "#Defining torch dataset class for poems\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8aecb5-c164-4b77-8849-4c686c2abb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_data_training(data, char_max_line = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e57386-ac09-497e-9040-b22699b38e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the vocabulary = 11340\n"
     ]
    }
   ],
   "source": [
    "pad_word = \"<pad>\"\n",
    "bos_word = \"<bos>\"\n",
    "eos_word = \"<eos>\"\n",
    "unk_word = \"<unk>\"\n",
    "sep_word = \"sep\"\n",
    "\n",
    "pad_id = 0\n",
    "bos_id = 1\n",
    "eos_id = 2\n",
    "unk_id = 3\n",
    "sep_id = 4\n",
    "    \n",
    "def normalize_sentence(s):\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word_to_id = {pad_word: pad_id, bos_word: bos_id, eos_word:eos_id, unk_word: unk_id, sep_word: sep_id}\n",
    "        self.word_count = {}\n",
    "        self.id_to_word = {pad_id: pad_word, bos_id: bos_word, eos_id: eos_word, unk_id: unk_word, sep_id: sep_word}\n",
    "        self.num_words = 5\n",
    "    \n",
    "    def get_ids_from_sentence(self, sentence):\n",
    "        sentence = normalize_sentence(sentence)\n",
    "        sent_ids = [bos_id] + [self.word_to_id[word.lower()] if word.lower() in self.word_to_id \\\n",
    "                               else unk_id for word in sentence.split()] + \\\n",
    "                               [eos_id]\n",
    "        return sent_ids\n",
    "    \n",
    "    def tokenized_sentence(self, sentence):\n",
    "        sent_ids = self.get_ids_from_sentence(sentence)\n",
    "        return [self.id_to_word[word_id] for word_id in sent_ids]\n",
    "\n",
    "    def decode_sentence_from_ids(self, sent_ids):\n",
    "        words = list()\n",
    "        for i, word_id in enumerate(sent_ids):\n",
    "            if word_id in [bos_id, eos_id, pad_id]:\n",
    "                # Skip these words\n",
    "                continue\n",
    "            else:\n",
    "                words.append(self.id_to_word[word_id])\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def add_words_from_sentence(self, sentence):\n",
    "        sentence = normalize_sentence(sentence)\n",
    "        for word in sentence.split():\n",
    "            if word not in self.word_to_id:\n",
    "                # add this word to the vocabulary\n",
    "                self.word_to_id[word] = self.num_words\n",
    "                self.id_to_word[self.num_words] = word\n",
    "                self.word_count[word] = 1\n",
    "                self.num_words += 1\n",
    "            else:\n",
    "                # update the word count\n",
    "                self.word_count[word] += 1\n",
    "\n",
    "vocab = Vocabulary()\n",
    "for src in df['text']:\n",
    "    vocab.add_words_from_sentence(src.lower())\n",
    "\n",
    "print(f\"Total words in the vocabulary = {vocab.num_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca140986-dc0a-446c-8ac4-c40832df65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poem_dataset(Dataset):\n",
    "    \"\"\"Single-Turn version of Cornell Movie Dialog Cropus dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, poems, context,vocab, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            conversations: list of tuple (src_string, tgt_string) \n",
    "                         - src_string: String of the source sentence\n",
    "                         - tgt_string: String of the target sentence\n",
    "            vocab: Vocabulary object that contains the mapping of \n",
    "                    words to indices\n",
    "            device: cpu or cuda\n",
    "        \"\"\"\n",
    "        l = []\n",
    "        \n",
    "        for i in range(len(poems)):\n",
    "            l.append( ( context[i] + ' sep ' + poems[i] , poems[i] ))\n",
    "        \n",
    "        self.conversations = l.copy()\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "\n",
    "        def encode(src, tgt):\n",
    "            src_ids = self.vocab.get_ids_from_sentence(src)\n",
    "            tgt_ids = self.vocab.get_ids_from_sentence(tgt)\n",
    "            return (src_ids, tgt_ids)\n",
    "\n",
    "        # We will pre-tokenize the conversations and save in id lists for later use\n",
    "        self.tokenized_conversations = [encode(src, tgt) for src, tgt in self.conversations]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return {\"conv_ids\":self.tokenized_conversations[idx], \"conv\":self.conversations[idx]}\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (src_seq, tgt_seq).\n",
    "    We should build a custom collate_fn rather than using default collate_fn,\n",
    "    because merging sequences (including padding) is not supported in default.\n",
    "    Seqeuences are padded to the maximum length of mini-batch sequences (dynamic padding).\n",
    "    Args:\n",
    "        data: list of dicts {\"conv_ids\":(src_ids, tgt_ids), \"conv\":(src_str, trg_str)}.\n",
    "            - src_ids: list of src piece ids; variable length.\n",
    "            - tgt_ids: list of tgt piece ids; variable length.\n",
    "            - src_str: String of src\n",
    "            - tgt_str: String of tgt\n",
    "    Returns: dict { \"conv_ids\":     (src_ids, tgt_ids), \n",
    "                    \"conv\":         (src_str, tgt_str), \n",
    "                    \"conv_tensors\": (src_seqs, tgt_seqs)}\n",
    "            src_seqs: torch tensor of shape (src_padded_length, batch_size).\n",
    "            tgt_seqs: torch tensor of shape (tgt_padded_length, batch_size).\n",
    "            src_padded_length = length of the longest src sequence from src_ids\n",
    "            tgt_padded_length = length of the longest tgt sequence from tgt_ids\n",
    "    \"\"\"\n",
    "    # Sort conv_ids based on decreasing order of the src_lengths.\n",
    "    # This is required for efficient GPU computations.\n",
    "    src_ids = [torch.LongTensor(e[\"conv_ids\"][0]) for e in data]\n",
    "    tgt_ids = [torch.LongTensor(e[\"conv_ids\"][1]) for e in data]\n",
    "    src_str = [e[\"conv\"][0] for e in data]\n",
    "    tgt_str = [e[\"conv\"][1] for e in data]\n",
    "    data = list(zip(src_ids, tgt_ids, src_str, tgt_str))\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    src_ids, tgt_ids, src_str, tgt_str = zip(*data)\n",
    "\n",
    "\n",
    "    # Pad the src_ids and tgt_ids using token pad_id to create src_seqs and tgt_seqs\n",
    "    \n",
    "    # Implementation tip: You can use the nn.utils.rnn.pad_sequence utility\n",
    "    # function to combine a list of variable-length sequences with padding.\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    src_seqs = nn.utils.rnn.pad_sequence(src_ids, padding_value = pad_id,\n",
    "                                         batch_first = False)\n",
    "    tgt_seqs = nn.utils.rnn.pad_sequence(tgt_ids, padding_value = pad_id, \n",
    "                                         batch_first = False)\n",
    "    \n",
    "    src_padded_length = len(src_seqs[0])\n",
    "    tgt_padded_length = len(tgt_seqs[0])\n",
    "    return {\"conv_ids\":(src_ids, tgt_ids), \"conv\":(src_str, tgt_str), \"conv_tensors\":(src_seqs.to(device), tgt_seqs.to(device))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2568c4-1691-438d-8032-4efce1c000b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for all_conversations\n",
    "\n",
    "all_poems = df['text'].tolist()\n",
    "context = df['context'].tolist()\n",
    "\n",
    "dataset = Poem_dataset(all_poems, context, vocab, device)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, \n",
    "                               shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84b29501-e2a3-4f45-a3e9-8287376d0326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Phoenix and the Turtle sep Let the bird of loudest lay\n",
      "['<bos>', 'the', 'phoenix', 'and', 'the', 'turtle', 'sep', 'let', 'the', 'bird', 'of', 'loudest', 'lay', '<eos>']\n",
      "[1, 6, 100, 17, 6, 101, 4, 5, 6, 7, 8, 9, 10, 2]\n",
      "the phoenix and the turtle sep let the bird of loudest lay\n",
      "\n",
      "The Phoenix and the Turtle sep On the sole Arabian tree\n",
      "['<bos>', 'the', 'phoenix', 'and', 'the', 'turtle', 'sep', 'on', 'the', 'sole', 'arabian', 'tree', '<eos>']\n",
      "[1, 6, 100, 17, 6, 101, 4, 11, 6, 12, 13, 14, 2]\n",
      "the phoenix and the turtle sep on the sole arabian tree\n",
      "\n",
      "The Phoenix and the Turtle sep Herald sad and trumpet be,\n",
      "['<bos>', 'the', 'phoenix', 'and', 'the', 'turtle', 'sep', 'herald', 'sad', 'and', 'trumpet', 'be', '<eos>']\n",
      "[1, 6, 100, 17, 6, 101, 4, 15, 16, 17, 18, 19, 2]\n",
      "the phoenix and the turtle sep herald sad and trumpet be\n",
      "\n",
      "Word = world\n",
      "Word ID = 392\n",
      "Word decoded from ID = world\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for src, tgt in dataset.conversations[:3]:\n",
    "    sentence = src\n",
    "    word_tokens = vocab.tokenized_sentence(sentence)\n",
    "    # Automatically adds bos_id and eos_id before and after sentence ids respectively\n",
    "    word_ids = vocab.get_ids_from_sentence(sentence)\n",
    "    print(sentence)\n",
    "    print(word_tokens)\n",
    "    print(word_ids)\n",
    "    print(vocab.decode_sentence_from_ids(word_ids))\n",
    "    print()\n",
    "\n",
    "word = \"world\"\n",
    "word_id = vocab.word_to_id[word.lower()]\n",
    "print(f\"Word = {word}\")\n",
    "print(f\"Word ID = {word_id}\")\n",
    "print(f\"Word decoded from ID = {vocab.decode_sentence_from_ids([word_id])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fd61011-e2e7-4315-94fb-ae4566803354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing first training batch of size 5\n",
      "List of source strings:\n",
      "Song: to Celia [Come, my Celia, let us prove] sep Fame and rumor are but toys.\n",
      "Prosopopoia: or Mother Hubbard's Tale sep Yet many eke of them (God wot) are driven\n",
      "Song: Sweetest love, I do not go sep Destiny may take thy part,\n",
      "Epithalamion sep The whyles the boyes run up and downe the street,\n",
      "Three Cantos sep Let us hear John Heydon!\n",
      "\n",
      "Tokenized source ids:\n",
      "tensor([   1,  362,   20, 6795,   40,  231, 6795,    5, 2724,  554,    4,  251,\n",
      "          17, 6800,  220,   27, 5685,   26,    2])\n",
      "tensor([   1,    3,  221,  428,    3,   36, 3326,    4,  124,  487,  442,    8,\n",
      "         132,  872,  852,  220, 3318,    2])\n",
      "tensor([   1,  362, 3461,   96,  235,  242,   41,   91,    4, 6401,  210,  785,\n",
      "          78, 3796,    2])\n",
      "tensor([    1, 10020,     4,     6,  2459,     6,  2414,  2460,  1008,    17,\n",
      "         1362,     6,  2461,     2])\n",
      "tensor([   1, 2404, 8447,    4,    5, 2724, 3333, 8189, 8599,  171,    2])\n",
      "\n",
      "Padded source ids as tensor (shape torch.Size([19, 5])):\n",
      "tensor([[    1,     1,     1,     1,     1],\n",
      "        [  362,     3,   362, 10020,  2404],\n",
      "        [   20,   221,  3461,     4,  8447],\n",
      "        [ 6795,   428,    96,     6,     4],\n",
      "        [   40,     3,   235,  2459,     5],\n",
      "        [  231,    36,   242,     6,  2724],\n",
      "        [ 6795,  3326,    41,  2414,  3333],\n",
      "        [    5,     4,    91,  2460,  8189],\n",
      "        [ 2724,   124,     4,  1008,  8599],\n",
      "        [  554,   487,  6401,    17,   171],\n",
      "        [    4,   442,   210,  1362,     2],\n",
      "        [  251,     8,   785,     6,     0],\n",
      "        [   17,   132,    78,  2461,     0],\n",
      "        [ 6800,   872,  3796,     2,     0],\n",
      "        [  220,   852,     2,     0,     0],\n",
      "        [   27,   220,     0,     0,     0],\n",
      "        [ 5685,  3318,     0,     0,     0],\n",
      "        [   26,     2,     0,     0,     0],\n",
      "        [    2,     0,     0,     0,     0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test one batch of training data\n",
    "first_batch = next(iter(data_loader))\n",
    "print(f\"Testing first training batch of size {len(first_batch['conv'][0])}\")\n",
    "print(f\"List of source strings:\")\n",
    "print_list(first_batch[\"conv\"][0])\n",
    "print(f\"Tokenized source ids:\")\n",
    "print_list(first_batch[\"conv_ids\"][0])\n",
    "print(f\"Padded source ids as tensor (shape {first_batch['conv_tensors'][0].size()}):\")\n",
    "print(first_batch[\"conv_tensors\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab4b33b3-dd1b-4d0a-8812-38495c9bad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Erato(nn.Module):\n",
    "    def __init__(self, vocab, emb_dim = 300, hidden_dim = 300, num_layers = 2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize your model's parameters here. To get started, we suggest\n",
    "        # setting all embedding and hidden dimensions to 300, using encoder and\n",
    "        # decoder GRUs with 2 layers, and using a dropout rate of 0.1.\n",
    "\n",
    "        # Implementation tip: To create a bidirectional GRU, you don't need to\n",
    "        # create two GRU networks. Instead use nn.GRU(..., bidirectional=True).\n",
    "        \n",
    "        self.num_words = num_words = vocab.num_words\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        # YOUR CODE HERE\n",
    "        self.encode_emb = nn.Embedding(self.num_words,self.emb_dim)\n",
    "        self.encode_gru = nn.GRU(self.emb_dim, self.hidden_dim,\n",
    "                          num_layers=self.num_layers, dropout=dropout,\n",
    "                          bidirectional=True,batch_first=False)\n",
    "        self.encode_l_hidden = nn.Linear(2*self.num_layers,self.num_layers)\n",
    "        self.encode_l_output = nn.Linear(2*self.hidden_dim,self.hidden_dim)\n",
    "        #self.relu = nn.ReLU()\n",
    "        #self.l2 = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.dropout_enc = nn.Dropout(dropout)\n",
    "\n",
    "        self.decode_emb = self.encode_emb\n",
    "        \n",
    "        self.decode_gru = nn.GRU(self.emb_dim, self.hidden_dim,\n",
    "                          num_layers=self.num_layers, dropout=dropout,\n",
    "                          bidirectional=False,batch_first=False)\n",
    "        self.d_l = nn.Linear(self.hidden_dim,self.num_words)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=2)\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "        self.dropout_dec = nn.Dropout(dropout)\n",
    "        \n",
    "        self.softmax_att = nn.Softmax(dim=0)\n",
    "        self.attention_matrix = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.attention_decode_cat = nn.Linear(2*self.hidden_dim,self.num_words)\n",
    "\n",
    "    def encode(self, source):\n",
    "        \"\"\"Encode the source batch using a bidirectional GRU encoder.\n",
    "\n",
    "        Args:\n",
    "            source: An integer tensor with shape (max_src_sequence_length,\n",
    "                batch_size) containing subword indices for the source sentences.\n",
    "\n",
    "        Returns:\n",
    "            A tuple with three elements:\n",
    "                encoder_output: The output hidden representation of the encoder \n",
    "                    with shape (max_src_sequence_length, batch_size, hidden_size).\n",
    "                    Can be obtained by adding the hidden representations of both \n",
    "                    directions of the encoder bidirectional GRU. \n",
    "                encoder_mask: A boolean tensor with shape (max_src_sequence_length,\n",
    "                    batch_size) indicating which encoder outputs correspond to padding\n",
    "                    tokens. Its elements should be True at positions corresponding to\n",
    "                    padding tokens and False elsewhere.\n",
    "                encoder_hidden: The final hidden states of the bidirectional GRU \n",
    "                    (after a suitable projection) that will be used to initialize \n",
    "                    the decoder. This should be a tensor h_n with shape \n",
    "                    (num_layers, batch_size, hidden_size). Note that the hidden \n",
    "                    state returned by the bi-GRU cannot be used directly. Its \n",
    "                    initial dimension is twice the required size because it \n",
    "                    contains state from two directions.\n",
    "\n",
    "        The first two return values are not required for the baseline model and will\n",
    "        only be used later in the attention model. If desired, they can be replaced\n",
    "        with None for the initial implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Implementation tip: consider using packed sequences to more easily work\n",
    "        # with the variable-length sequences represented by the source tensor.\n",
    "        # See https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence.\n",
    "\n",
    "        # https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
    "\n",
    "        # Implementation tip: there are many simple ways to combine the forward\n",
    "        # and backward portions of the final hidden state, e.g. addition, averaging,\n",
    "        # or a linear transformation of the appropriate size. Any of these\n",
    "        # should let you reach the required performance.\n",
    "\n",
    "        # Compute a tensor containing the length of each source sequence.\n",
    "        source_lengths = torch.sum(source != pad_id, axis=0).cpu()\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        emb = self.dropout_enc(self.encode_emb(source))\n",
    "        emb = nn.utils.rnn.pack_padded_sequence(emb, source_lengths,\n",
    "                                                enforce_sorted = False)\n",
    "        encoder_output, encoder_hidden = self.encode_gru(emb)\n",
    "        encoder_output,_ = nn.utils.rnn.pad_packed_sequence(encoder_output,\n",
    "                                                   padding_value=pad_id)\n",
    "        #encoder_hidden = self.l2(self.relu(self.l1(encoder_hidden)))\n",
    "        #encoder_hidden = torch.tanh(encoder_hidden)\n",
    "        encoder_output = self.encode_l_output(encoder_output)\n",
    "        \n",
    "        encoder_hidden = self.encode_l_hidden(encoder_hidden.permute(2,1,0))\n",
    "        encoder_hidden = encoder_hidden.permute(2,1,0).contiguous()\n",
    "        # Compute the encoder mask\n",
    "        encoder_mask = (source == pad_id)\n",
    "\n",
    "        return encoder_output, encoder_mask.type(torch.bool), encoder_hidden\n",
    "\n",
    "    def decode(self, decoder_input, last_hidden, encoder_output, encoder_mask):\n",
    "        \"\"\"Run the decoder GRU for one decoding step from the last hidden state.\n",
    "\n",
    "        The third and fourth arguments are not used in the baseline model, but are\n",
    "        included for compatibility with the attention model in the next section.\n",
    "\n",
    "        Args:\n",
    "            decoder_input: An integer tensor with shape (1, batch_size) containing \n",
    "                the subword indices for the current decoder input.\n",
    "            last_hidden: A pair of tensors h_{t-1} representing the last hidden\n",
    "                state of the decoder, each with shape (num_layers, batch_size,\n",
    "                hidden_size). For the first decoding step the last_hidden will be \n",
    "                encoder's final hidden representation.\n",
    "            encoder_output: The output of the encoder with shape\n",
    "                (max_src_sequence_length, batch_size, hidden_size).\n",
    "            encoder_mask: The output mask from the encoder with shape\n",
    "                (max_src_sequence_length, batch_size). Encoder outputs at positions\n",
    "                with a True value correspond to padding tokens and should be ignored.\n",
    "\n",
    "        Returns:\n",
    "            A tuple with three elements:\n",
    "                logits: A tensor with shape (batch_size,\n",
    "                    vocab_size) containing unnormalized scores for the next-word\n",
    "                    predictions at each position.\n",
    "                decoder_hidden: tensor h_n with the same shape as last_hidden \n",
    "                    representing the updated decoder state after processing the \n",
    "                    decoder input.\n",
    "                attention_weights: A tensor with shape (batch_size, \n",
    "                    max_src_sequence_length) representing the normalized\n",
    "                    attention weights. This should sum to 1 along the last dimension.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        emb = self.dropout_dec(self.decode_emb(decoder_input))\n",
    "        decoder_output, decoder_hidden = self.decode_gru(emb,last_hidden)\n",
    "        b = decoder_output.squeeze(0)\n",
    "\n",
    "        # I use the General method (Luong2015) for attention\n",
    "        encoder_output = encoder_output.masked_fill(encoder_mask.unsqueeze(2),0)\n",
    "        att = torch.matmul(self.attention_matrix(decoder_output.permute(1,0,2)),\n",
    "                           encoder_output.permute(1,2,0))\n",
    "        att = att.squeeze(1).permute(1,0)\n",
    "        \n",
    "        att = att.masked_fill(encoder_mask, float(\"-inf\"))\n",
    "        att = self.softmax_att(att)\n",
    "        c = att.unsqueeze(2) * encoder_output\n",
    "        c = torch.sum(c,0)\n",
    "        logits = self.attention_decode_cat(torch.cat((b,c),1))\n",
    "        return (logits, decoder_hidden, att)\n",
    "\n",
    "    def compute_loss(self, source, target):\n",
    "        \"\"\"Run the model on the source and compute the loss on the target.\n",
    "\n",
    "        Args:\n",
    "            source: An integer tensor with shape (max_source_sequence_length,\n",
    "                batch_size) containing subword indices for the source sentences.\n",
    "            target: An integer tensor with shape (max_target_sequence_length,\n",
    "                batch_size) containing subword indices for the target sentences.\n",
    "\n",
    "        Returns:\n",
    "            A scalar float tensor representing cross-entropy loss on the current batch\n",
    "            divided by the number of target tokens in the batch.\n",
    "            Many of the target tokens will be pad tokens. You should mask the loss \n",
    "            from these tokens using appropriate mask on the target tokens loss.\n",
    "        \"\"\"\n",
    "\n",
    "        # Implementation tip: don't feed the target tensor directly to the decoder.\n",
    "        # To see why, note that for a target sequence like <s> A B C </s>, you would\n",
    "        # want to run the decoder on the prefix <s> A B C and have it predict the\n",
    "        # suffix A B C </s>.\n",
    "\n",
    "        # You may run self.encode() on the source only once and decode the target \n",
    "        # one step at a time.\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        max_source_sequence_length = target.shape[0]\n",
    "        local_batch_size = target.shape[1]\n",
    "        encoder_output, encoder_mask, h = self.encode(source)\n",
    "        input_decode = target[0,:].unsqueeze(0)\n",
    "        outputs = bos_id*torch.ones(1,local_batch_size,self.num_words, requires_grad=True).cuda()\n",
    "        for t in range(1,max_source_sequence_length):\n",
    "            out,h,_ = self.decode(input_decode, h, encoder_output, encoder_mask)\n",
    "            input_decode = target[t,:].unsqueeze(0)\n",
    "            outputs = torch.cat((outputs,out.unsqueeze(0)),0)\n",
    "        return self.loss(outputs[1:].reshape((max_source_sequence_length-1)*local_batch_size,self.num_words),target[1:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c01483e-0a77-43d9-b785-90301515be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs, model_file, learning_rate=0.0001):\n",
    "    \"\"\"Train the model for given µnumber of epochs and save the trained model in \n",
    "    the final model_file.\n",
    "    \"\"\"\n",
    "\n",
    "    decoder_learning_ratio = 5.0\n",
    "    #encoder_parameter_names = ['word_embedding', 'encoder']\n",
    "    encoder_parameter_names = ['encode_emb', 'encode_gru', 'l1', 'l2']\n",
    "                           \n",
    "    encoder_named_params = list(filter(lambda kv: any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
    "    decoder_named_params = list(filter(lambda kv: not any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
    "    encoder_params = [e[1] for e in encoder_named_params]\n",
    "    decoder_params = [e[1] for e in decoder_named_params]\n",
    "    optimizer = torch.optim.AdamW([{'params': encoder_params},\n",
    "                {'params': decoder_params, 'lr': learning_rate * decoder_learning_ratio}], lr=learning_rate)\n",
    "    \n",
    "    clip = 50.0\n",
    "    for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
    "        # print(f\"Total training instances = {len(train_dataset)}\")\n",
    "        # print(f\"train_data_loader = {len(train_data_loader)} {1180 > len(train_data_loader)/20}\")\n",
    "        with tqdm.notebook.tqdm(\n",
    "                data_loader,\n",
    "                desc=\"epoch {}\".format(epoch + 1),\n",
    "                unit=\"batch\",\n",
    "                total=len(data_loader)) as batch_iterator:\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for i, batch_data in enumerate(batch_iterator, start=1):\n",
    "                source, target = batch_data[\"conv_tensors\"]\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.compute_loss(source, target)\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                # Gradient clipping before taking the step\n",
    "                _ = nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_iterator.set_postfix(mean_loss=total_loss / i, current_loss=loss.item())\n",
    "    # Save the model after training         \n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3250c3a-5cd5-45df-99ee-bfa97cdc34a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f107a87138224a9eae0db192474d2ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36e4309edeb41f9a3745bc3c9106af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 1:   0%|          | 0/425 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1357a5f4694752991e1720e154efe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 2:   0%|          | 0/425 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db48a9b81a604c3ba95c3b43b7b62eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 3:   0%|          | 0/425 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e556b143bf64fe5ac329ebdffc891f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 4:   0%|          | 0/425 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab81b16293a4006bd6758d629703e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 5:   0%|          | 0/425 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8045db3480f941ac8e24bb8dd3d3a52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 6:   0%|          | 0/425 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135dd8bb892748a486fa0422231726d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 7:   0%|          | 0/425 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You are welcome to adjust these parameters based on your model implementation.\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "# Reloading the data_loader to increase batch_size\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, \n",
    "                               shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "baseline_model = Erato(vocab).to(device)\n",
    "train(baseline_model, data_loader, num_epochs, \"baseline_model.pt\",learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a35318-02d0-46b1-9be2-e57f82b556e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_beam(model, sentence, k=5, max_length=100):\n",
    "    \"\"\"Make predictions for the given inputs using beam search.\n",
    "    \n",
    "    Args:\n",
    "        model: A sequence-to-sequence model.\n",
    "        sentence: An input sentence, represented as string.\n",
    "        k: The size of the beam.\n",
    "        max_length: The maximum length at which to truncate outputs in order to\n",
    "            avoid non-terminating inference.\n",
    "    \n",
    "    Returns:\n",
    "        A list of k beam predictions. Each element in the list should be a string\n",
    "        corresponding to one of the top k predictions for the corresponding input,\n",
    "        sorted in descending order by its final score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Implementation tip: once an eos_token has been generated for any beam, \n",
    "    # remove its subsequent predictions from that beam by adding a small negative \n",
    "    # number like -1e9 to the appropriate logits. This will ensure that the \n",
    "    # candidates are removed from the beam, as its probability will be very close\n",
    "    # to 0. Using this method, uou will be able to reuse the beam of an already \n",
    "    # finished candidate\n",
    "\n",
    "    # Implementation tip: while you are encouraged to keep your tensor dimensions\n",
    "    # constant for simplicity (aside from the sequence length), some special care\n",
    "    # will need to be taken on the first iteration to ensure that your beam\n",
    "    # doesn't fill up with k identical copies of the same candidate.\n",
    "    \n",
    "    # You are welcome to tweak alpha\n",
    "    alpha = 0.\n",
    "    model.eval()\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    sentence_ids = torch.tensor(vocab.get_ids_from_sentence(sentence)).cuda()\n",
    "    sentence_ids = sentence_ids.unsqueeze(1)\n",
    "    encoder_output, encoder_mask, h = model.encode(sentence_ids)\n",
    "\n",
    "    out_start = sentence_ids[0]\n",
    "    beam = [out_start for i in range(k)]\n",
    "    beam_scores = [1 for i in range(k)]\n",
    "    hiddens = [h for i in range(k)]\n",
    "    generations = []\n",
    "    generations_scores = []\n",
    "    curr_l = 0\n",
    "    eos_tensor = torch.Tensor([eos_id]).int().cuda()\n",
    "    while beam:\n",
    "        logits = torch.Tensor().cuda()\n",
    "        inds = torch.Tensor().int().cuda()\n",
    "        curr_k = len(beam)\n",
    "        if curr_l==max_length:\n",
    "            for i in range(curr_k):\n",
    "                  generations += [torch.cat((beam[i],eos_tensor),0)]\n",
    "                  generations_scores += [new_beam_scores[i]]\n",
    "            break\n",
    "        else:\n",
    "            for i in range(curr_k):\n",
    "                out, hiddens[i], _ = model.decode(beam[i][-1].view(1,1), hiddens[i], encoder_output,\n",
    "                                     encoder_mask)\n",
    "                logit,ind = torch.topk(out.squeeze(), curr_k, dim=0)\n",
    "                logits = torch.cat((logits,logit),0)\n",
    "                inds = torch.cat((inds,ind),0)\n",
    "            new_beam = []\n",
    "            new_beam_scores = []\n",
    "            new_hiddens = []\n",
    "            if curr_l==0:\n",
    "                for i in range(curr_k):\n",
    "                    max_ind = torch.argmax(nn.functional.log_softmax(logit,dim=0))\n",
    "                    new_beam_scores += [float(logit[max_ind])]\n",
    "                    logit[max_ind] = -1e9\n",
    "                    new_beam += [torch.cat((beam[0],ind[max_ind].unsqueeze(0)),0)]\n",
    "                    new_hiddens += [hiddens[0]]\n",
    "            else:\n",
    "                top_logits,top_inds_logit = torch.topk(torch.repeat_interleave(torch.Tensor(beam_scores).cuda(),\n",
    "                                                                               curr_k)\\\n",
    "                                                       +nn.functional.log_softmax(logits,dim=0),\n",
    "                                                       curr_k, dim=0)\n",
    "                for i in range(curr_k):\n",
    "                    if inds[top_inds_logit[i]]==eos_id:\n",
    "                        generations += [torch.cat((beam[top_inds_logit[i]//curr_k],inds[top_inds_logit[i]].unsqueeze(0)),0)]\n",
    "                        generations_scores+=[float(logits[top_inds_logit[i]])/(generations[-1].shape[0]**alpha)]\n",
    "                    else:\n",
    "                        new_beam += [torch.cat((beam[top_inds_logit[i]//curr_k],inds[top_inds_logit[i]].unsqueeze(0)),0)]\n",
    "                        new_hiddens += [hiddens[top_inds_logit[i]//curr_k]]\n",
    "                        new_beam_scores += [float(logits[top_inds_logit[i]])]\n",
    "            beam = new_beam\n",
    "            beam_scores = new_beam_scores\n",
    "            hiddens = new_hiddens\n",
    "        curr_l +=1\n",
    "    generations = [g for _, g in sorted(zip(generations_scores, generations))]\n",
    "    generations.reverse()\n",
    "    return [vocab.decode_sentence_from_ids(s.tolist()) for s in generations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21378c8-877d-4094-95b8-6a0247c1b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch \n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7dd2b-73bd-4eeb-be0b-263bbbbd3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"under the sea\"\n",
    "model = baseline_model\n",
    "predict_beam(model, sentence, k=5, max_length=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
